{
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install git+https://github.com/brianbt/btorch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S63IgFLE1SF7",
    "outputId": "851107b0-8c96-4512-c7b0-1b2efb404df5",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:13:45.411881Z",
     "iopub.execute_input": "2022-10-05T05:13:45.413141Z",
     "iopub.status.idle": "2022-10-05T05:14:12.304387Z",
     "shell.execute_reply.started": "2022-10-05T05:13:45.413022Z",
     "shell.execute_reply": "2022-10-05T05:14:12.303188Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.12.1+cu116)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.13.1+cu116)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.12.1+cu116)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.6.15.2)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mCollecting git+https://github.com/brianbt/btorch\n  Cloning https://github.com/brianbt/btorch to /tmp/pip-req-build-xw2tpa1r\n  Running command git clone --filter=blob:none --quiet https://github.com/brianbt/btorch /tmp/pip-req-build-xw2tpa1r\n  Resolved https://github.com/brianbt/btorch to commit af13b057b343a724ab9ab6d5baa685c1d62fc54b\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (1.12.1+cu116)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (0.13.1+cu116)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (1.21.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (1.3.5)\nRequirement already satisfied: torchinfo in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (1.7.1)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (4.5.4.60)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (1.0.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (3.5.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (4.64.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from btorch==0.0.1) (1.7.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->btorch==0.0.1) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->btorch==0.0.1) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->btorch==0.0.1) (0.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->btorch==0.0.1) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->btorch==0.0.1) (9.1.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->btorch==0.0.1) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->btorch==0.0.1) (1.4.3)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->btorch==0.0.1) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->btorch==0.0.1) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->btorch==0.0.1) (1.0.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->btorch==0.0.1) (4.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->btorch==0.0.1) (2.28.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->btorch==0.0.1) (1.15.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->btorch==0.0.1) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->btorch==0.0.1) (2022.6.15.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->btorch==0.0.1) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->btorch==0.0.1) (2.1.0)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# !nvcc --version"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-10-05T05:14:12.306787Z",
     "iopub.execute_input": "2022-10-05T05:14:12.307489Z",
     "iopub.status.idle": "2022-10-05T05:14:12.314078Z",
     "shell.execute_reply.started": "2022-10-05T05:14:12.307444Z",
     "shell.execute_reply": "2022-10-05T05:14:12.311513Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import btorch\n",
    "import btorch.nn as nn\n",
    "import btorch.nn.functional as F\n",
    "from btorch.vision import models\n",
    "from btorch.vision.utils import pplot, img_MinMaxScaler\n",
    "from btorch.utils.trainer import get_lr\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "id": "oV2cdmZ7088m",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:14:12.315608Z",
     "iopub.execute_input": "2022-10-05T05:14:12.316050Z",
     "iopub.status.idle": "2022-10-05T05:14:13.158370Z",
     "shell.execute_reply.started": "2022-10-05T05:14:12.316014Z",
     "shell.execute_reply": "2022-10-05T05:14:13.157343Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Dataset, CIFAR10"
   ],
   "metadata": {
    "id": "Dh3KzaSt4ZxY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load CIFAR10 dataset, do augmentation on the trainset\n",
    "class MultiTransform():\n",
    "    def __init__(self, transforms, n=2):\n",
    "        self.transforms = transforms\n",
    "        self.n = n\n",
    "    def __call__(self, x):\n",
    "        return [self.transforms(x) for _ in range(self.n)]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "  transforms.RandomResizedCrop(size=64),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "  transforms.RandomGrayscale(p=0.2),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "transform_test = transforms.Compose([\n",
    "  transforms.Resize(64),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "trainset = datasets.CIFAR10('./cifar10',train=True, download=True, transform=MultiTransform(transform_train))\n",
    "testset = datasets.CIFAR10('./cifar10',train=False, download=True, transform=transform_test)\n",
    "\n",
    "transform_clf_train = transforms.Compose([\n",
    "  transforms.Resize(64),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "trainset_clf = datasets.CIFAR10('./cifar10',train=True, download=True, transform=transform_clf_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSwlkLqf1Fna",
    "outputId": "546933f4-aeb0-4417-ea73-5a0e359dc930",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:14:13.160977Z",
     "iopub.execute_input": "2022-10-05T05:14:13.161858Z",
     "iopub.status.idle": "2022-10-05T05:14:15.731888Z",
     "shell.execute_reply.started": "2022-10-05T05:14:13.161819Z",
     "shell.execute_reply": "2022-10-05T05:14:15.730185Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# for i in trainset:\n",
    "#   pplot(img_MinMaxScaler(torch.stack(i[0])))\n",
    "#   # pplot(img_MinMaxScaler(i[0][0]))\n",
    "#   # pplot(img_MinMaxScaler(i[0][1]))\n",
    "#   break"
   ],
   "metadata": {
    "id": "jOhZj6lF4iON",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:14:15.737424Z",
     "iopub.execute_input": "2022-10-05T05:14:15.738134Z",
     "iopub.status.idle": "2022-10-05T05:14:15.746880Z",
     "shell.execute_reply.started": "2022-10-05T05:14:15.738086Z",
     "shell.execute_reply": "2022-10-05T05:14:15.745776Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# for i in train_loader:\n",
    "#   print(i[0][0].shape)\n",
    "#   break\n",
    "# torch.concat(i[0]).shape"
   ],
   "metadata": {
    "id": "PR53JY2d9JoI",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:14:15.749831Z",
     "iopub.execute_input": "2022-10-05T05:14:15.751251Z",
     "iopub.status.idle": "2022-10-05T05:14:15.758873Z",
     "shell.execute_reply.started": "2022-10-05T05:14:15.751203Z",
     "shell.execute_reply": "2022-10-05T05:14:15.757587Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define InfoNCE Loss"
   ],
   "metadata": {
    "id": "AiNKY4_M9C9z",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def info_nce_loss(views, temperature=0.07):\n",
    "    \"\"\"edit from https://github.com/sthalles/SimCLR/blob/master/simclr.py#L26\n",
    "\n",
    "    Args:\n",
    "        views(List[Tensor]): must have at least two elements\n",
    "        temperature(float): temperature\n",
    "\n",
    "    Returns:\n",
    "        Tensor(N*n_views, n_views-1), Tensor(N*n_views,)\n",
    "      \n",
    "    Note:\n",
    "        You should do CrossEntropyLoss() on the output logit and labels\n",
    "\n",
    "    Examples:\n",
    "        >>> features = model(x)\n",
    "        >>> features_aug = model(x_aug)\n",
    "        >>> logits, labels = info_nce_loss([features, features_aug])\n",
    "        >>> loss = lossfn(logits, labels)\n",
    "        >>> loss.backward()\n",
    "    \"\"\"\n",
    "    assert len(views) >= 2\n",
    "    n_views = len(views)\n",
    "    batch_size = views[0].shape[0]\n",
    "    features = torch.cat(views, dim=0)\n",
    "\n",
    "    labels = torch.cat([torch.arange(batch_size) for i in range(n_views)], dim=0) #(batch_size*n_views)\n",
    "    labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float() #(batch_size*n_views)\n",
    "    labels = labels.to(views[0].device)\n",
    "    # Below two line is same as cos_similarity \n",
    "    features = F.normalize(features, dim=1) #(N*n_views,D)\n",
    "    similarity_matrix = torch.matmul(features, features.T) #(N*n_views,D)\n",
    "\n",
    "    # assert similarity_matrix.shape == (\n",
    "    #     self.args.n_views * self.args.batch_size, self.args.n_views * self.args.batch_size)\n",
    "    # assert similarity_matrix.shape == labels.shape\n",
    "\n",
    "    # discard the main diagonal from both: labels and similarities matrix\n",
    "    mask = torch.eye(labels.shape[0], dtype=torch.bool, device=views[0].device) #(N*n_views*N*n_views), diagonal matrix\n",
    "    labels = labels[~mask].view(labels.shape[0], -1)#(N*n_views,N*n_views-N), labels without diagonal\n",
    "    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)#(N*n_views,N*n_views-N), sim_matrix without diagonal\n",
    "    # Above labels and similarity is the one without diagonal, without self, only the positive-pair remain\n",
    "\n",
    "    # assert similarity_matrix.shape == labels.shape\n",
    "    # select and combine multiple positives\n",
    "    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1) #(N*n_view,n_view-1)\n",
    "    # select only the negatives the negatives\n",
    "    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1) #(N*n_view,N*n_view-n_view)\n",
    "\n",
    "    logits = torch.cat([positives, negatives], dim=1) #(N*n_view,N*n_view-1)\n",
    "    # Now each data point have (N*n_view-1) values\n",
    "    # The positive pair's similarity is in the first place\n",
    "    # Therefore in labels we set all as `0`, refering to the first index.\n",
    "    labels = torch.zeros(logits.shape[0], dtype=torch.long, device=views[0].device) #(N*n_view)\n",
    "    \n",
    "    logits = logits / temperature\n",
    "    return logits, labels"
   ],
   "metadata": {
    "id": "TCZtKoKk9BgB",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:14:15.761578Z",
     "iopub.execute_input": "2022-10-05T05:14:15.763653Z",
     "iopub.status.idle": "2022-10-05T05:14:15.789677Z",
     "shell.execute_reply.started": "2022-10-05T05:14:15.763410Z",
     "shell.execute_reply": "2022-10-05T05:14:15.788062Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create SimCLR Module\n",
    "\n",
    "This step is to train a Resnet50 model in constractive learning way.  \n",
    "So that the Resnet50 backbone(exclude fc layers) will output a emebedding that is similar for each class."
   ],
   "metadata": {
    "id": "eRczJ-Bs-Z1U",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.backbone = models.resnet50()\n",
    "        backbone_out_dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(nn.Linear(backbone_out_dim, backbone_out_dim),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Linear(backbone_out_dim, embedding_dim))\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    @classmethod\n",
    "    def train_epoch(cls, net, criterion, trainloader, optimizer, epoch_idx, device='cuda', config=None, **kwargs):\n",
    "        \"\"\"This is the very basic training function for one epoch. Override this function when necessary\n",
    "\n",
    "        Returns:\n",
    "            (float or dict): train_loss\n",
    "        \"\"\"\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(enumerate(trainloader), total=len(trainloader),\n",
    "                    disable=(kwargs.get(\"verbose\", 1) == 0))\n",
    "        batch_idx = 1\n",
    "        for batch_idx, (inputs, _) in pbar:\n",
    "            inputs_1 = inputs[0].to(device)\n",
    "            inputs_2 = inputs[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            features_1 = net(inputs_1)\n",
    "            features_2 = net(inputs_1)\n",
    "            logits, labels = info_nce_loss([features_1, features_2])\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"epoch {epoch_idx + 1} iter {batch_idx}: train loss {loss.item():.5f}, lr:{get_lr(optimizer)}\")\n",
    "        return train_loss / (batch_idx + 1)"
   ],
   "metadata": {
    "id": "Pnf9FzWvQeO6",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:14:15.792776Z",
     "iopub.execute_input": "2022-10-05T05:14:15.794050Z",
     "iopub.status.idle": "2022-10-05T05:14:15.807126Z",
     "shell.execute_reply.started": "2022-10-05T05:14:15.793987Z",
     "shell.execute_reply": "2022-10-05T05:14:15.806110Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fit Model"
   ],
   "metadata": {
    "id": "3NJ06xke-kz-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Model\n",
    "net = SimCLR(64)\n",
    "\n",
    "# Loss & Optimizer & Config\n",
    "net._config['max_epoch'] = 50\n",
    "\n",
    "# net._optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "net._optimizer = torch.optim.Adam(net.parameters(), lr=3e-4, weight_decay=0)\n",
    "net._lossfn = nn.CrossEntropyLoss()\n",
    "net._lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(net._optimizer, net._config['max_epoch'])\n",
    "\n",
    "# Set GPU\n",
    "device = net.auto_gpu()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwbHcoUuoEtb",
    "outputId": "309ff414-60d3-4cba-8110-dd894dd05d88",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:39:55.001426Z",
     "iopub.execute_input": "2022-10-05T05:39:55.002410Z",
     "iopub.status.idle": "2022-10-05T05:39:55.501321Z",
     "shell.execute_reply.started": "2022-10-05T05:39:55.002368Z",
     "shell.execute_reply": "2022-10-05T05:39:55.500259Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "auto_gpu: using GPU (Tesla P100-PCIE-16GB)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Fit\n",
    "net.train()\n",
    "net.fit(trainset, batch_size=256, workers=2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EggdBt5m-6HZ",
    "outputId": "a99c73b2-6e01-488d-f0f9-f36768ae4773",
    "execution": {
     "iopub.status.busy": "2022-10-05T05:39:55.504393Z",
     "iopub.execute_input": "2022-10-05T05:39:55.504686Z",
     "iopub.status.idle": "2022-10-05T07:20:28.734924Z",
     "shell.execute_reply.started": "2022-10-05T05:39:55.504660Z",
     "shell.execute_reply": "2022-10-05T07:20:28.733642Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "text": "epoch 1 iter 195: train loss 0.00183, lr:0.0003: 100%|██████████| 196/196 [01:59<00:00,  1.64it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 0: Training loss: 0.9489260352684699. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 2 iter 195: train loss 0.00084, lr:0.0002997040092642407: 100%|██████████| 196/196 [02:00<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1: Training loss: 0.004472448309700537. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 3 iter 195: train loss 0.00067, lr:0.0002988172051971717: 100%|██████████| 196/196 [02:01<00:00,  1.61it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 2: Training loss: 0.0033208432569339567. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 4 iter 195: train loss 0.00071, lr:0.0002973430876093033: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 3: Training loss: 0.0026420809341029133. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 5 iter 195: train loss 0.00064, lr:0.00029528747416929463: 100%|██████████| 196/196 [02:00<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 4: Training loss: 0.0024812590334996848. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 6 iter 195: train loss 0.00077, lr:0.00029265847744427303: 100%|██████████| 196/196 [01:59<00:00,  1.64it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 5: Training loss: 0.0023309272349864357. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 7 iter 195: train loss 0.00060, lr:0.0002894664728832377: 100%|██████████| 196/196 [01:59<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 6: Training loss: 0.00283090786964452. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 8 iter 195: train loss 0.00048, lr:0.000285724057869903: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 7: Training loss: 0.00225473985795231. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 9 iter 195: train loss 0.00044, lr:0.00028144600200657956: 100%|██████████| 196/196 [01:59<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 8: Training loss: 0.0023135714793021847. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 10 iter 195: train loss 4.48425, lr:0.0002766491888253023: 100%|██████████| 196/196 [01:58<00:00,  1.65it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 9: Training loss: 1.6973177327730513. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 11 iter 195: train loss 1.11238, lr:0.00027135254915624217: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 10: Training loss: 3.1901362599158776. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 12 iter 195: train loss 0.20571, lr:0.0002655769864163684: 100%|██████████| 196/196 [01:59<00:00,  1.64it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 11: Training loss: 1.2793329505591977. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 13 iter 195: train loss 0.06433, lr:0.0002593452941132117: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 12: Training loss: 0.35865254422687753. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 14 iter 195: train loss 0.03247, lr:0.0002526820658893033: 100%|██████████| 196/196 [02:01<00:00,  1.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 13: Training loss: 0.16458026870933115. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 15 iter 195: train loss 0.01763, lr:0.0002456135984623034: 100%|██████████| 196/196 [01:59<00:00,  1.64it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 14: Training loss: 0.0969867362279673. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 16 iter 195: train loss 0.01276, lr:0.00023816778784387096: 100%|██████████| 196/196 [02:01<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 15: Training loss: 0.061829440704337796. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 17 iter 195: train loss 0.00988, lr:0.00023037401924684944: 100%|██████████| 196/196 [02:00<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 16: Training loss: 0.049119215434873284. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 18 iter 195: train loss 0.01214, lr:0.00022226305111525726: 100%|██████████| 196/196 [02:01<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 17: Training loss: 0.04012687615004881. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 19 iter 195: train loss 0.00731, lr:0.00021386689373476087: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 18: Training loss: 0.034514376950659315. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 20 iter 195: train loss 0.01178, lr:0.00020521868290270169: 100%|██████████| 196/196 [02:01<00:00,  1.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 19: Training loss: 0.02937475035005078. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 21 iter 195: train loss 0.00987, lr:0.00019635254915624208: 100%|██████████| 196/196 [02:01<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 20: Training loss: 0.026286430181745365. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 22 iter 195: train loss 0.00651, lr:0.00018730348307472816: 100%|██████████| 196/196 [02:01<00:00,  1.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 21: Training loss: 0.02355909750948907. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 23 iter 195: train loss 0.00429, lr:0.00017810719718785867: 100%|██████████| 196/196 [02:01<00:00,  1.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 22: Training loss: 0.020752144972698724. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 24 iter 195: train loss 0.00600, lr:0.00016879998503464564: 100%|██████████| 196/196 [02:01<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 23: Training loss: 0.018386130736741637. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 25 iter 195: train loss 0.01019, lr:0.000159418577929397: 100%|██████████| 196/196 [02:01<00:00,  1.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 24: Training loss: 0.017005472743351544. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 26 iter 195: train loss 0.00263, lr:0.00015: 100%|██████████| 196/196 [02:01<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 25: Training loss: 0.015420429805312686. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 27 iter 195: train loss 0.00352, lr:0.000140581422070603: 100%|██████████| 196/196 [02:01<00:00,  1.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 26: Training loss: 0.014441455951986872. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 28 iter 195: train loss 0.00308, lr:0.00013120001496535436: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 27: Training loss: 0.012971626730084571. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 29 iter 195: train loss 0.00258, lr:0.0001218928028121413: 100%|██████████| 196/196 [02:02<00:00,  1.61it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 28: Training loss: 0.012193633845712686. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 30 iter 195: train loss 0.00279, lr:0.00011269651692527177: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 29: Training loss: 0.011359415246335295. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 31 iter 195: train loss 0.00262, lr:0.00010364745084375793: 100%|██████████| 196/196 [02:01<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 30: Training loss: 0.010603567615046869. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 32 iter 195: train loss 0.00318, lr:9.478131709729836e-05: 100%|██████████| 196/196 [01:58<00:00,  1.65it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 31: Training loss: 0.01040323208589867. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 33 iter 195: train loss 0.00264, lr:8.61331062652391e-05: 100%|██████████| 196/196 [01:57<00:00,  1.66it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 32: Training loss: 0.009573523248593343. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 34 iter 195: train loss 0.00176, lr:7.773694888474269e-05: 100%|██████████| 196/196 [01:56<00:00,  1.68it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 33: Training loss: 0.008850147145828803. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 35 iter 195: train loss 0.00182, lr:6.962598075315047e-05: 100%|██████████| 196/196 [01:58<00:00,  1.65it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 34: Training loss: 0.00844692959561849. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 36 iter 195: train loss 0.00157, lr:6.183221215612905e-05: 100%|██████████| 196/196 [01:57<00:00,  1.66it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 35: Training loss: 0.00791871914585425. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 37 iter 195: train loss 0.00179, lr:5.438640153769654e-05: 100%|██████████| 196/196 [01:57<00:00,  1.66it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 36: Training loss: 0.007590804088680188. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 38 iter 195: train loss 0.00271, lr:4.73179341106967e-05: 100%|██████████| 196/196 [02:03<00:00,  1.58it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 37: Training loss: 0.00715035921181267. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 39 iter 195: train loss 0.00122, lr:4.0654705886788306e-05: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 38: Training loss: 0.006650804351345275. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 40 iter 195: train loss 0.00198, lr:3.4423013583631635e-05: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 39: Training loss: 0.006526797892269203. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 41 iter 195: train loss 0.00159, lr:2.86474508437579e-05: 100%|██████████| 196/196 [01:59<00:00,  1.64it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 40: Training loss: 0.006095707170636754. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 42 iter 195: train loss 0.00185, lr:2.3350811174697774e-05: 100%|██████████| 196/196 [01:59<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 41: Training loss: 0.005978802536799554. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 43 iter 195: train loss 0.00109, lr:1.8553997993420465e-05: 100%|██████████| 196/196 [02:04<00:00,  1.58it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 42: Training loss: 0.00583311538176364. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 44 iter 195: train loss 0.00165, lr:1.42759421300971e-05: 100%|██████████| 196/196 [02:02<00:00,  1.60it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 43: Training loss: 0.005811780183314707. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 45 iter 195: train loss 0.00129, lr:1.05335271167623e-05: 100%|██████████| 196/196 [02:02<00:00,  1.60it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 44: Training loss: 0.005803785179931746. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 46 iter 195: train loss 0.00139, lr:7.341522555726972e-06: 100%|██████████| 196/196 [02:02<00:00,  1.60it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 45: Training loss: 0.00542200424373435. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 47 iter 195: train loss 0.00144, lr:4.712525830705356e-06: 100%|██████████| 196/196 [02:02<00:00,  1.60it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 46: Training loss: 0.005162035111499456. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 48 iter 195: train loss 0.00138, lr:2.656912390696692e-06: 100%|██████████| 196/196 [02:00<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 47: Training loss: 0.005229836742617. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 49 iter 195: train loss 0.00179, lr:1.1827948028283352e-06: 100%|██████████| 196/196 [02:00<00:00,  1.63it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 48: Training loss: 0.005206644658607488. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nepoch 50 iter 195: train loss 0.00115, lr:2.9599073575926616e-07: 100%|██████████| 196/196 [02:00<00:00,  1.62it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 49: Training loss: 0.00519908163447066. Testing loss: Not Provided\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "net.backbone.fc[0].in_features"
   ],
   "metadata": {
    "id": "8Dhhp59d6eCA",
    "execution": {
     "iopub.status.busy": "2022-10-05T07:20:28.737360Z",
     "iopub.execute_input": "2022-10-05T07:20:28.738098Z",
     "iopub.status.idle": "2022-10-05T07:20:28.746321Z",
     "shell.execute_reply.started": "2022-10-05T07:20:28.738043Z",
     "shell.execute_reply": "2022-10-05T07:20:28.745187Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "execution_count": 17,
     "output_type": "execute_result",
     "data": {
      "text/plain": "2048"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use SimCLR to train classifier\n",
    "\n",
    "Now we have trained the backbone's embedding in constractive learning way.  \n",
    "We would like to use this pre-trained backbone (exclude fc layers) to train a classifier and do prediction."
   ],
   "metadata": {
    "id": "KV39JwPe430L",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Resnet_constractive(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_class):\n",
    "        super(Resnet_constractive, self).__init__()\n",
    "        fc_in_features = net.backbone.fc[0].in_features\n",
    "        self.backbone = pretrained_model\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.fc = nn.Linear(fc_in_features, num_class)\n",
    "        btorch.utils.trainer.freeze(self.backbone)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "id": "aQgWPk1X-7J1",
    "execution": {
     "iopub.status.busy": "2022-10-05T07:20:28.749287Z",
     "iopub.execute_input": "2022-10-05T07:20:28.749703Z",
     "iopub.status.idle": "2022-10-05T07:20:28.759214Z",
     "shell.execute_reply.started": "2022-10-05T07:20:28.749664Z",
     "shell.execute_reply": "2022-10-05T07:20:28.758037Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classifier = Resnet_constractive(net.backbone, 10)\n",
    "classifier.compile(torch.optim.Adam(classifier.parameters(), lr=3e-4),\n",
    "                   torch.nn.CrossEntropyLoss())\n",
    "\n",
    "# Set GPU\n",
    "device = classifier.auto_gpu()"
   ],
   "metadata": {
    "id": "anKV8awg6zik",
    "execution": {
     "iopub.status.busy": "2022-10-05T07:34:02.332651Z",
     "iopub.execute_input": "2022-10-05T07:34:02.333075Z",
     "iopub.status.idle": "2022-10-05T07:34:02.346500Z",
     "shell.execute_reply.started": "2022-10-05T07:34:02.333038Z",
     "shell.execute_reply": "2022-10-05T07:34:02.345322Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": "auto_gpu: using GPU (Tesla P100-PCIE-16GB)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "classifier.fit(trainset_clf, validation_data=testset, epochs=20, scoring='accuarcy', batch_size=256)"
   ],
   "metadata": {
    "id": "YQxfW8XM62Q4",
    "execution": {
     "iopub.status.busy": "2022-10-05T07:41:49.042686Z",
     "iopub.execute_input": "2022-10-05T07:41:49.043155Z",
     "iopub.status.idle": "2022-10-05T07:55:58.732631Z",
     "shell.execute_reply.started": "2022-10-05T07:41:49.043114Z",
     "shell.execute_reply": "2022-10-05T07:55:58.730715Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.7/site-packages/btorch/nn/modules/module.py:237: UserWarning: x might not support <class 'torchvision.datasets.cifar.CIFAR10'>. It will treat x as ``Dataset``.\n  f\"x might not support {type(x)}. It will treat x as ``Dataset``.\")\nepoch 1 iter 195: train loss 2.18137, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.24it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 0: Training loss: 2.2176359685099856. Testing loss: {'loss': 2.143305603504181, 'score': 0.2169}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 2 iter 195: train loss 2.10966, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.17it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1: Training loss: 2.114233581387267. Testing loss: {'loss': 2.0952429695129395, 'score': 0.2369}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 3 iter 195: train loss 2.01267, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.24it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 2: Training loss: 2.082659593042062. Testing loss: {'loss': 2.0790544986724853, 'score': 0.2454}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 4 iter 195: train loss 1.94667, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.23it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 3: Training loss: 2.064533140586347. Testing loss: {'loss': 2.059746022987366, 'score': 0.2531}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 5 iter 195: train loss 2.06743, lr:0.0003: 100%|██████████| 196/196 [00:26<00:00,  7.30it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 4: Training loss: 2.0451320014437853. Testing loss: {'loss': 2.0489746041297914, 'score': 0.2618}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 6 iter 195: train loss 2.08933, lr:0.0003: 100%|██████████| 196/196 [00:26<00:00,  7.30it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 5: Training loss: 2.0287997138743497. Testing loss: {'loss': 2.036337102985382, 'score': 0.2588}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 7 iter 195: train loss 2.00929, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.25it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 6: Training loss: 2.0191808756516902. Testing loss: {'loss': 2.037413045310974, 'score': 0.2698}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 8 iter 195: train loss 2.15047, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.25it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 7: Training loss: 2.011448270204116. Testing loss: {'loss': 2.01534072971344, 'score': 0.2704}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 9 iter 195: train loss 1.89696, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.19it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 8: Training loss: 1.9943080228202197. Testing loss: {'loss': 1.9963886588096618, 'score': 0.2791}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 10 iter 195: train loss 1.92942, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.21it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 9: Training loss: 1.986403765118852. Testing loss: {'loss': 1.9894114495277404, 'score': 0.2794}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 11 iter 195: train loss 2.13339, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.25it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 10: Training loss: 1.9813672553519814. Testing loss: {'loss': 1.9873397064208984, 'score': 0.2817}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 12 iter 195: train loss 2.03947, lr:0.0003: 100%|██████████| 196/196 [00:26<00:00,  7.28it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 11: Training loss: 1.967572661078706. Testing loss: {'loss': 1.9737695108413695, 'score': 0.2955}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 13 iter 195: train loss 2.01788, lr:0.0003: 100%|██████████| 196/196 [00:26<00:00,  7.26it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 12: Training loss: 1.966944660459246. Testing loss: {'loss': 1.9662916718482972, 'score': 0.2918}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 14 iter 195: train loss 2.24480, lr:0.0003: 100%|██████████| 196/196 [00:26<00:00,  7.32it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 13: Training loss: 1.9593812105607014. Testing loss: {'loss': 1.971524469947815, 'score': 0.2944}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 15 iter 195: train loss 1.96551, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.26it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 14: Training loss: 1.951015133638771. Testing loss: {'loss': 1.967513446521759, 'score': 0.2851}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 16 iter 195: train loss 1.95065, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.18it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 15: Training loss: 1.942461378720342. Testing loss: {'loss': 1.9597594560623168, 'score': 0.2947}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 17 iter 195: train loss 2.12038, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.23it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 16: Training loss: 1.9449528820660649. Testing loss: {'loss': 1.9575307704925538, 'score': 0.2927}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 18 iter 195: train loss 1.82288, lr:0.0003: 100%|██████████| 196/196 [00:27<00:00,  7.22it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 17: Training loss: 1.9359692900764698. Testing loss: {'loss': 1.9559478386878968, 'score': 0.2902}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 19 iter 195: train loss 1.85459, lr:0.0003: 100%|██████████| 196/196 [00:26<00:00,  7.33it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 18: Training loss: 1.932980623172254. Testing loss: {'loss': 1.9756568343162537, 'score': 0.2856}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "epoch 20 iter 195: train loss 1.92412, lr:0.0003: 100%|██████████| 196/196 [00:26<00:00,  7.31it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 19: Training loss: 1.929997804821754. Testing loss: {'loss': 1.94420458650589, 'score': 0.2963}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}