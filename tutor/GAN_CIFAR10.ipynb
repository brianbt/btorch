{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/brianbt/btorch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"Uv4jpJXWXEBe","outputId":"37b688ae-b11d-495c-c525-a2f2ece2151f","execution":{"iopub.status.busy":"2022-07-03T11:38:32.015101Z","iopub.execute_input":"2022-07-03T11:38:32.015632Z","iopub.status.idle":"2022-07-03T11:38:46.623995Z","shell.execute_reply.started":"2022-07-03T11:38:32.015520Z","shell.execute_reply":"2022-07-03T11:38:46.622883Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport btorch\nfrom btorch import nn\nimport btorch.nn.functional as F\nfrom btorch.vision.utils import UnNormalize\nfrom torchvision import transforms, datasets\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt","metadata":{"id":"E4q3lpCGXEBg","execution":{"iopub.status.busy":"2022-07-03T11:38:46.626700Z","iopub.execute_input":"2022-07-03T11:38:46.627086Z","iopub.status.idle":"2022-07-03T11:38:48.648128Z","shell.execute_reply.started":"2022-07-03T11:38:46.627048Z","shell.execute_reply":"2022-07-03T11:38:48.647174Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset, CIFAR10","metadata":{"id":"uXdU0yvdASsJ"}},{"cell_type":"code","source":"# Load CIFAR10 dataset, do augmentation on the trainset\ntransform_train = transforms.Compose([\n  transforms.Resize(64),\n  transforms.RandomHorizontalFlip(0.5),\n  transforms.ToTensor(),\n  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n  ])\ntransform_test = transforms.Compose([\n  transforms.Resize(64),\n  transforms.ToTensor(),\n  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n  ])\ntrainset = datasets.CIFAR10('./cifar10',train=True, download=True, transform=transform_train)\ntestset = datasets.CIFAR10('./cifar10',train=False, download=True, transform=transform_test)\n\n# # Only select the `dog` class\n# train_idx = torch.tensor(trainset.targets,dtype=torch.long) == 7\n# test_idx = torch.tensor(testset.targets,dtype=torch.long) == 7\n# trainset.targets = torch.tensor(trainset.targets,dtype=torch.long)[train_idx]\n# trainset.data = trainset.data[train_idx]\n# testset.targets = torch.tensor(testset.targets,dtype=torch.long)[test_idx]\n# testset.data = testset.data[test_idx]","metadata":{"id":"5By3T92OXEBh","outputId":"4a62c210-aefc-45a8-fabc-c76db081efdc","execution":{"iopub.status.busy":"2022-07-03T11:38:48.649493Z","iopub.execute_input":"2022-07-03T11:38:48.650144Z","iopub.status.idle":"2022-07-03T11:38:56.435331Z","shell.execute_reply.started":"2022-07-03T11:38:48.650106Z","shell.execute_reply":"2022-07-03T11:38:56.434373Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Create GAN Model","metadata":{"id":"qsG8X7-WXEBi"}},{"cell_type":"markdown","source":"## Generator and Discriminator","metadata":{"id":"wWaa9VIkAYTM"}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, latent_dim):\n        super(Generator, self).__init__()\n        self.latent_dim = latent_dim\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, 64 * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n    def forward(self,x):\n        return self.main(x)\n    def sample(self, batch_size = 1):\n        noise = torch.randn((batch_size, self.latent_dim, 1, 1), device=self.device())\n        return self.forward(noise)","metadata":{"id":"dF57FMCOXEBj","execution":{"iopub.status.busy":"2022-07-03T11:39:25.546816Z","iopub.execute_input":"2022-07-03T11:39:25.549101Z","iopub.status.idle":"2022-07-03T11:39:25.563425Z","shell.execute_reply.started":"2022-07-03T11:39:25.549059Z","shell.execute_reply":"2022-07-03T11:39:25.562406Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n    def forward(self,x):\n        return self.main(x)\n        ","metadata":{"id":"KsxFn3fcXEBk","execution":{"iopub.status.busy":"2022-07-03T11:39:25.568455Z","iopub.execute_input":"2022-07-03T11:39:25.571240Z","iopub.status.idle":"2022-07-03T11:39:25.585237Z","shell.execute_reply.started":"2022-07-03T11:39:25.571201Z","shell.execute_reply":"2022-07-03T11:39:25.584108Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Lets test the Generator and Discriminator output","metadata":{"id":"3oYp6-YaAdMW"}},{"cell_type":"code","source":"g = Generator(100)\nprint(g.summary(input_size = (16,100,1,1)))\nd = Discriminator()\nprint(d.summary(input_size=(16,3,64,64)))\nprint(d(g.sample(1)).shape)","metadata":{"id":"wZWcBaBWXEBk","outputId":"1c685902-68ce-42b9-da05-9e2cd3786e6e","execution":{"iopub.status.busy":"2022-07-03T11:39:25.591087Z","iopub.execute_input":"2022-07-03T11:39:25.593670Z","iopub.status.idle":"2022-07-03T11:39:25.871228Z","shell.execute_reply.started":"2022-07-03T11:39:25.593633Z","shell.execute_reply":"2022-07-03T11:39:25.870208Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Create GAN Module","metadata":{"id":"E84zxXlyAoKo"}},{"cell_type":"code","source":"class GAN(nn.Module):\n    def __init__(self, latent_dim):\n        super(GAN, self).__init__()\n        self.latent_dim = latent_dim\n        self.g=Generator(latent_dim)\n        self.d=Discriminator()\n    def forward(self,x):\n        return self.d(x)\n    def sample(self, batch_size):\n        return self.g.sample(batch_size)\n    \n    @classmethod\n    def train_epoch(cls, net, criterion, trainloader, optimizer, epoch_idx, device='cuda', config=None,**kwargs):\n        \"\"\"This is the very basic training function for one epoch. Override this function when necessary\n            \n        Returns:\n            (float): train_loss\n        \"\"\"\n        net.g.train()\n        net.d.train()\n        G_loss = 0\n        D_loss = 0\n        G_curr_loss = 0 \n        D_curr_loss = torch.nan\n        pbar = tqdm(enumerate(trainloader), total=len(trainloader), disable=(kwargs.get(\"verbose\", 1)==0))\n        for batch_idx, (inputs, _) in pbar:\n            # Trian G ###############################\n            optimizer['G'].zero_grad()\n            fake_inputs = net.sample(inputs.shape[0])\n            fool_labels = torch.ones(inputs.shape[0], device=net.device())\n            fool_predicted = net.d(fake_inputs).view(-1)\n            G_fool_loss = criterion(fool_predicted, fool_labels)\n            G_fool_loss.backward()\n            \n            optimizer['G'].step()\n            G_curr_loss = G_fool_loss.item()\n            G_loss += G_curr_loss\n            \n            # Trian D ###############################\n            if epoch_idx >= 0: # train Discriminator less\n                optimizer['D'].zero_grad()\n                ## Train with real data\n                inputs = inputs.to(device)\n                real_labels = torch.ones(inputs.shape[0], device=net.device())\n                real_predicted = net.d(inputs).view(-1)\n                D_real_loss = criterion(real_predicted, real_labels)\n\n                ## Train with fake data\n                fake_labels = torch.zeros(inputs.shape[0], device=net.device())\n                fake_predicted = net.d(fake_inputs.detach()).view(-1)\n                D_fake_loss = criterion(fake_predicted, fake_labels)\n\n                D_curr_lossB = D_real_loss+D_fake_loss\n                D_curr_lossB.backward()\n                optimizer['D'].step()\n                \n                D_curr_loss = D_curr_lossB.item()\n                D_loss = D_loss + D_curr_loss\n            \n            pbar.set_description(\n                f\"epoch {epoch_idx+1} iter {batch_idx}: D loss {D_curr_loss:.5f}, G loss {G_curr_loss:.5f}.\")\n        return {'D_loss': D_loss/(batch_idx+1), 'G_loss': G_loss/(batch_idx+1)}\n\n    @classmethod\n    def before_each_train_epoch(cls, net, criterion, optimizer, trainloader, testloader=None, epoch_idx=0, lr_scheduler=None, config=None,**kwargs):\n      config['evol'].append(net.g(config['evol_seed']))\n\n    @classmethod\n    def test_epoch(cls, net, criterion, testloader, epoch_idx=0, scoring=None, device='cuda', config=None,**kwargs):\n        \"\"\"This is the very basic evaluating function for one epoch. Override this function when necessary\n            \n        Returns:\n            (float): eval_loss\n        \"\"\"\n        net.g.eval()\n        net.d.eval()\n        G_loss = 0\n        D_loss = 0\n        with torch.inference_mode():\n            for batch_idx, (inputs, targets) in enumerate(testloader):\n                # Test G ###############################\n                fake_inputs = net.sample(inputs.shape[0])\n                fool_labels = torch.ones(inputs.shape[0], device=net.device())\n                fool_predicted = net.d(fake_inputs).view(-1)\n                G_fool_loss = criterion(fool_predicted, fool_labels)\n                G_loss += G_fool_loss.item()\n                \n                \n                # Test D ###############################\n                if epoch_idx >= 0:\n                    ## Test with real data\n                    inputs = inputs.to(device)\n                    real_labels = torch.ones(inputs.shape[0], device=net.device())\n                    real_predicted = net.d(inputs).view(-1)\n                    D_real_loss = criterion(real_predicted, real_labels)\n\n                    ## Test with fake data\n                    fake_labels = torch.zeros(inputs.shape[0], device=net.device())\n                    fake_predicted = net.d(fake_inputs).view(-1)\n                    D_fake_loss = criterion(fake_predicted, fake_labels)\n\n                    D_loss = D_loss + (D_real_loss.item() + D_fake_loss.item())/2\n\n                \n        return {'D_loss': D_loss/(batch_idx+1), 'G_loss': G_loss/(batch_idx+1)}","metadata":{"id":"CCw9fO6HXEBl","execution":{"iopub.status.busy":"2022-07-03T11:39:25.978194Z","iopub.execute_input":"2022-07-03T11:39:25.980128Z","iopub.status.idle":"2022-07-03T11:39:26.016583Z","shell.execute_reply.started":"2022-07-03T11:39:25.980080Z","shell.execute_reply":"2022-07-03T11:39:26.015116Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Paper init weight as (0, 0.02)","metadata":{"id":"L1tXrv8WAXM9"}},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"id":"TIQJsLGXsKdq","execution":{"iopub.status.busy":"2022-07-03T11:39:27.284938Z","iopub.execute_input":"2022-07-03T11:39:27.285986Z","iopub.status.idle":"2022-07-03T11:39:27.292993Z","shell.execute_reply.started":"2022-07-03T11:39:27.285942Z","shell.execute_reply":"2022-07-03T11:39:27.291875Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Init Model","metadata":{"id":"GcfZOSMIAuIi"}},{"cell_type":"code","source":"from btorch.utils.load_save import save_model, resume\n# Model\nlatent_dim = 128\ngan = GAN(latent_dim)\ngan.g.apply(weights_init)\ngan.d.apply(weights_init)\n\n# Loss & Optimizer & Config\ngan._lossfn = nn.BCELoss()\ngan._optimizer = {'D':torch.optim.Adam(gan.d.parameters(), lr=0.0002, betas=(0.5, 0.999)),\n                  'G':torch.optim.Adam(gan.g.parameters(), lr=0.0002, betas=(0.5, 0.999))}\ngan._config['max_epoch'] = 50\ngan._config['val_freq'] = 1\ngan._config['evol'] = []\ngan._config['evol_seed'] = torch.randn((25,latent_dim,1,1), device='cuda')\n# gan._config['save'] = './checkpoints/'\n# gan._config['save_every_epoch_checkpoint'] = 20\n# gan._config['save_base_on'] = 'G_loss'\n# gan._config['tensorboard'] = '500epochs'\n\n\n# Set GPU\ngan.auto_gpu()","metadata":{"id":"HwmSrzKyXEBn","outputId":"ed7b95b0-a4c8-4dd7-9ebc-d40f9b1fff9f","execution":{"iopub.status.busy":"2022-07-03T11:39:32.526718Z","iopub.execute_input":"2022-07-03T11:39:32.527426Z","iopub.status.idle":"2022-07-03T11:39:32.652532Z","shell.execute_reply.started":"2022-07-03T11:39:32.527388Z","shell.execute_reply":"2022-07-03T11:39:32.651543Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"iOiORx-uAyCL"}},{"cell_type":"code","source":"gan.fit(trainset, validation_data=testset, batch_size=512, drop_last=True, verbose=1, workers=4)","metadata":{"id":"LhV5wBh2XEBo","outputId":"c3b6342a-dd48-4934-e499-a34dacabebe5","execution":{"iopub.status.busy":"2022-07-03T11:39:34.098939Z","iopub.execute_input":"2022-07-03T11:39:34.099285Z","iopub.status.idle":"2022-07-03T12:10:23.945522Z","shell.execute_reply.started":"2022-07-03T11:39:34.099257Z","shell.execute_reply":"2022-07-03T12:10:23.943568Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Plot the Loss","metadata":{"id":"n55H3crPAzaQ"}},{"cell_type":"code","source":"import pandas as pd\nprint('train_loss')\npd.DataFrame(gan._history[0]['train_loss_data']).plot()\nplt.show()\nprint('test_loss')\npd.DataFrame(gan._history[0]['test_loss_data']).plot()\nplt.show()","metadata":{"id":"LZFvbqpp3ke_","outputId":"0b6be0d6-a0b5-4174-e5bd-275259c1527a","execution":{"iopub.status.busy":"2022-07-03T12:13:34.376633Z","iopub.execute_input":"2022-07-03T12:13:34.377017Z","iopub.status.idle":"2022-07-03T12:13:34.781680Z","shell.execute_reply.started":"2022-07-03T12:13:34.376986Z","shell.execute_reply":"2022-07-03T12:13:34.780771Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Generated Images","metadata":{"id":"mTY3UTaJm2wg"}},{"cell_type":"code","source":"gan.eval()\ngan.g.eval()\ngan.d.eval()\ngenerated = gan.sample(1)\nbtorch.vision.utils.pplot(btorch.vision.utils.img_MinMaxScaler(generated))\nprint(gan.d(generated))","metadata":{"id":"pV811l9zXEBo","outputId":"d36e53d8-38c0-4810-c3cf-a4a89bc08cdb","execution":{"iopub.status.busy":"2022-07-03T12:13:36.328633Z","iopub.execute_input":"2022-07-03T12:13:36.329575Z","iopub.status.idle":"2022-07-03T12:13:36.533319Z","shell.execute_reply.started":"2022-07-03T12:13:36.329523Z","shell.execute_reply":"2022-07-03T12:13:36.532182Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Discriminator on Real Image","metadata":{"id":"swIQNRHnnb89"}},{"cell_type":"code","source":"gan.eval()\ngan.g.eval()\ngan.d.eval()\nfor i in trainset:\n  btorch.vision.utils.pplot(btorch.vision.utils.img_MinMaxScaler(i[0][0]))\n  print(gan.d(i[0].unsqueeze(0).cuda())[0])\n  break","metadata":{"id":"JsLymbPlXEBp","outputId":"958a8f55-daa6-4d1d-a482-e4763d260d66","execution":{"iopub.status.busy":"2022-07-03T12:13:37.011620Z","iopub.execute_input":"2022-07-03T12:13:37.012004Z","iopub.status.idle":"2022-07-03T12:13:37.186381Z","shell.execute_reply.started":"2022-07-03T12:13:37.011972Z","shell.execute_reply":"2022-07-03T12:13:37.185462Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Evolution of Generator","metadata":{"id":"HLk8rQfE6OLQ"}},{"cell_type":"code","source":"for i in range(0,len(gan._config['evol']), 5):\n  print(f\"epoch {i}\")\n  btorch.vision.utils.pplot(btorch.vision.utils.img_MinMaxScaler(gan._config['evol'][i]))","metadata":{"id":"uajIqSgWjdot","outputId":"d9dc9d66-d605-4be7-ff4b-bb75fbb7a6ca","execution":{"iopub.status.busy":"2022-07-03T12:13:54.549709Z","iopub.execute_input":"2022-07-03T12:13:54.550723Z","iopub.status.idle":"2022-07-03T12:14:12.340536Z","shell.execute_reply.started":"2022-07-03T12:13:54.550675Z","shell.execute_reply":"2022-07-03T12:14:12.339503Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"vP4gc_REAE51"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}