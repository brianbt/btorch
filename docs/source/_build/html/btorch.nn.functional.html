<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>btorch.nn.functional package &mdash; btorch latest documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> btorch
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">btorch.nn.functional package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-btorch.nn.functional.functional">btorch.nn.functional.functional module</a></li>
<li><a class="reference internal" href="#module-btorch.nn.functional">Module contents</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">btorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>btorch.nn.functional package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/btorch.nn.functional.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="btorch-nn-functional-package">
<h1>btorch.nn.functional package<a class="headerlink" href="#btorch-nn-functional-package" title="Permalink to this headline"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
</section>
<section id="module-btorch.nn.functional.functional">
<span id="btorch-nn-functional-functional-module"></span><h2>btorch.nn.functional.functional module<a class="headerlink" href="#module-btorch.nn.functional.functional" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.functional.adaptive_avg_pool2d_threshold">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.functional.</span></span><span class="sig-name descname"><span class="pre">adaptive_avg_pool2d_threshold</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_abs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.functional.adaptive_avg_pool2d_threshold" title="Permalink to this definition"></a></dt>
<dd><p>Avg Pool 2D threshold version. Only the values that are larger than threshold will be used to calc the avgerage value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – same as <cite>F.adaptive_avg_pool2d()</cite></p></li>
<li><p><strong>output_size</strong> – same as <cite>F.adaptive_avg_pool2d()</cite></p></li>
<li><p><strong>threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – Defaults to 1e-8.</p></li>
<li><p><strong>use_abs</strong> (<em>bool</em><em>, </em><em>optional</em>) – only <cite>torch.abs(input)&gt;threshold</cite> will be used to calc the average value, if True.
only <cite>input&gt;threshold</cite> will be used to calc the average value, if False.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>adaptive_avg_pool2d_threshold(*, threshold=0)</cite> should be same as <cite>F.adaptive_avg_pool2d(*)</cite></p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.functional.adaptive_pad1d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.functional.</span></span><span class="sig-name descname"><span class="pre">adaptive_pad1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.functional.adaptive_pad1d" title="Permalink to this definition"></a></dt>
<dd><p>pad a 1D Tensor to desire shape, pad right first if shape is odd.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – pytorch Tensor (<a href="#id1"><span class="problematic" id="id2">*</span></a>, D)</p></li>
<li><p><strong>shape</strong> (<em>List</em><em>(</em><em>int</em><em>)</em>) – desired (<a href="#id3"><span class="problematic" id="id4">*</span></a>, Dd). Only the last value will be used</p></li>
<li><p><strong>mode</strong> – see <cite>torch.nn.functional.pad</cite></p></li>
<li><p><strong>value</strong> – see <cite>torch.nn.functional.pad</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>pytorch Tensor (<a href="#id5"><span class="problematic" id="id6">*</span></a>, Dd)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.functional.adaptive_pad2d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.functional.</span></span><span class="sig-name descname"><span class="pre">adaptive_pad2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.functional.adaptive_pad2d" title="Permalink to this definition"></a></dt>
<dd><p>pad a 2D Tensor to desire shape, pad bottom/right first if shape is odd.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – pytorch Tensor (<a href="#id7"><span class="problematic" id="id8">*</span></a>, H, W)</p></li>
<li><p><strong>shape</strong> (<em>List</em><em>(</em><em>int</em><em>)</em>) – desired (<a href="#id9"><span class="problematic" id="id10">*</span></a>, Hd, Wd). Only the last two value will be used</p></li>
<li><p><strong>mode</strong> – see <cite>torch.nn.functional.pad</cite></p></li>
<li><p><strong>value</strong> – see <cite>torch.nn.functional.pad</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>pytorch Tensor (<a href="#id11"><span class="problematic" id="id12">*</span></a>, Hd, Wd)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.functional.adaptive_pad3d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.functional.</span></span><span class="sig-name descname"><span class="pre">adaptive_pad3d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.functional.adaptive_pad3d" title="Permalink to this definition"></a></dt>
<dd><p>pad a 3D Tensor to desire shape</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>Tensor</em>) – pytorch Tensor (<a href="#id13"><span class="problematic" id="id14">*</span></a>, D1, D2, D3)</p></li>
<li><p><strong>shape</strong> (<em>List</em><em>(</em><em>int</em><em>)</em>) – desired (<a href="#id15"><span class="problematic" id="id16">*</span></a>, D1d, D2d, D3d). Only the last three value will be used</p></li>
<li><p><strong>mode</strong> – see <cite>torch.nn.functional.pad</cite></p></li>
<li><p><strong>value</strong> – see <cite>torch.nn.functional.pad</cite></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>pytorch Tensor (<a href="#id17"><span class="problematic" id="id18">*</span></a>, D1d, D2d, D3d)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.functional.approx_count_nonzero">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.functional.</span></span><span class="sig-name descname"><span class="pre">approx_count_nonzero</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ell</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.functional.approx_count_nonzero" title="Permalink to this definition"></a></dt>
<dd><p>differentiable version of torch.count_nonzero()</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.functional.approx_where">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.functional.</span></span><span class="sig-name descname"><span class="pre">approx_where</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">condition</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.functional.approx_where" title="Permalink to this definition"></a></dt>
<dd><p>differentiable version of torch.where()</p>
<p>Usage same torch.where(a&gt;condition, x, y)
The closer <cite>eps</cite> to zero, the more accurate on cell that are a&lt;=condition (replaced by y)
a&gt;condition (replaced by x) does not guarantee, try increase eps to find the balance
<cite>eps</cite> should be &gt; 0</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.functional.multi_hot">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.functional.</span></span><span class="sig-name descname"><span class="pre">multi_hot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ls</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.functional.multi_hot" title="Permalink to this definition"></a></dt>
<dd><p>Take in List and turn it to multi-hot encoding</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html">https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html</a> for details
This function is similar to sklearn.preprocessing.MultiLabelBinarizer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ls</strong> (<em>List</em><em>[</em><em>int</em><em> or </em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em>) – List contains the class(es) of that element</p></li>
<li><p><strong>num_classes</strong> (<em>int</em><em>, </em><em>optional</em>) – Total number of classes.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>multi-hot encoding</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>LongTensor</p>
</dd>
</dl>
<dl class="simple">
<dt>Notice:</dt><dd><p>This function can be used to replace F.one_hot()
This may be slow if there are many element has more than one class</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input can be either in these three format</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,[</span><span class="mi">9</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">arr</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multi_hot</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-btorch.nn.functional">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-btorch.nn.functional" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.adaptive_avg_pool1d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">adaptive_avg_pool1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.adaptive_avg_pool1d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool1d</span></code> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size (single integer)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.avg_pool1d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">avg_pool1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.avg_pool1d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool1d</span></code> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>kernel_size</strong> – the size of the window. Can be a single number or a
tuple <cite>(kW,)</cite></p></li>
<li><p><strong>stride</strong> – the stride of the window. Can be a single number or a tuple
<cite>(sW,)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padW,)</cite>. Default: 0</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the
output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.celu_">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">celu_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.celu_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-func docutils literal notranslate"><span class="pre">celu()</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.channel_shuffle">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">channel_shuffle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.channel_shuffle" title="Permalink to this definition"></a></dt>
<dd><p>Divide the channels in a tensor of shape <span class="math notranslate nohighlight">\((*, C , H, W)\)</span>
into g groups and rearrange them as <span class="math notranslate nohighlight">\((*, C \frac g, g, H, W)\)</span>,
while keeping the original tensor shape.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">ChannelShuffle</span></code> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – the input tensor</p></li>
<li><p><strong>groups</strong> (<em>int</em>) – number of groups to divide channels in and rearrange.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">[[[[1, 2],</span>
<span class="go">   [3, 4]],</span>
<span class="go">  [[5, 6],</span>
<span class="go">   [7, 8]],</span>
<span class="go">  [[9, 10],</span>
<span class="go">   [11, 12]],</span>
<span class="go">  [[13, 14],</span>
<span class="go">   [15, 16]],</span>
<span class="go"> ]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">channel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">[[[[1, 2],</span>
<span class="go">   [3, 4]],</span>
<span class="go">  [[9, 10],</span>
<span class="go">   [11, 12]],</span>
<span class="go">  [[5, 6],</span>
<span class="go">   [7, 8]],</span>
<span class="go">  [[13, 14],</span>
<span class="go">   [15, 16]],</span>
<span class="go"> ]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.conv1d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">conv1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.conv1d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>This operator supports <span class="xref std std-ref">TensorFloat32</span>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span> <span class="pre">True</span></code>. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math notranslate nohighlight">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math notranslate nohighlight">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or
a one-element tuple <cite>(sW,)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – <p>implicit paddings on both sides of the input. Can be a string {‘valid’, ‘same’},
single number or a one-element tuple <cite>(padW,)</cite>. Default: 0
<code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> is the same as no padding. <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> pads
the input so the output has the shape as the input. However, this mode
doesn’t support any stride values other than 1.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code>, if the <code class="docutils literal notranslate"><span class="pre">weight</span></code> is even-length and
<code class="docutils literal notranslate"><span class="pre">dilation</span></code> is odd in any dimension, a full <code class="xref py py-func docutils literal notranslate"><span class="pre">pad()</span></code> operation
may be needed internally. Lowering performance.</p>
</div>
</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a one-element tuple <cite>(dW,)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.conv2d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">conv2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.conv2d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 2D convolution over an input image composed of several input
planes.</p>
<p>This operator supports <span class="xref std std-ref">TensorFloat32</span>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span> <span class="pre">True</span></code>. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math notranslate nohighlight">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias tensor of shape <span class="math notranslate nohighlight">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – <p>implicit paddings on both sides of the input. Can be a string {‘valid’, ‘same’},
single number or a tuple <cite>(padH, padW)</cite>. Default: 0
<code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> is the same as no padding. <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> pads
the input so the output has the shape as the input. However, this mode
doesn’t support any stride values other than 1.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code>, if the <code class="docutils literal notranslate"><span class="pre">weight</span></code> is even-length and
<code class="docutils literal notranslate"><span class="pre">dilation</span></code> is odd in any dimension, a full <code class="xref py py-func docutils literal notranslate"><span class="pre">pad()</span></code> operation
may be needed internally. Lowering performance.</p>
</div>
</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dH, dW)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.conv3d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">conv3d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.conv3d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 3D convolution over an input image composed of several input
planes.</p>
<p>This operator supports <span class="xref std std-ref">TensorFloat32</span>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span> <span class="pre">True</span></code>. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math notranslate nohighlight">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kT , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias tensor of shape <span class="math notranslate nohighlight">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – <p>implicit paddings on both sides of the input. Can be a string {‘valid’, ‘same’},
single number or a tuple <cite>(padT, padH, padW)</cite>. Default: 0
<code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> is the same as no padding. <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> pads
the input so the output has the shape as the input. However, this mode
doesn’t support any stride values other than 1.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code>, if the <code class="docutils literal notranslate"><span class="pre">weight</span></code> is even-length and
<code class="docutils literal notranslate"><span class="pre">dilation</span></code> is odd in any dimension, a full <code class="xref py py-func docutils literal notranslate"><span class="pre">pad()</span></code> operation
may be needed internally. Lowering performance.</p>
</div>
</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.conv_tbc">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">conv_tbc</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btorch.nn.functional.conv_tbc" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 1-dimensional sequence convolution over an input sequence.
Input and output dimensions are (Time, Batch, Channels) - hence TBC.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{sequence length} \times batch \times \text{in\_channels})\)</span></p></li>
<li><p><strong>weight</strong> – filter of shape (<span class="math notranslate nohighlight">\(\text{kernel width} \times \text{in\_channels} \times \text{out\_channels}\)</span>)</p></li>
<li><p><strong>bias</strong> – bias of shape (<span class="math notranslate nohighlight">\(\text{out\_channels}\)</span>)</p></li>
<li><p><strong>pad</strong> – number of timesteps to pad. Default: 0</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.conv_transpose1d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">conv_transpose1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.conv_transpose1d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>This operator supports <span class="xref std std-ref">TensorFloat32</span>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span> <span class="pre">True</span></code>. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math notranslate nohighlight">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math notranslate nohighlight">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sW,)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padW,)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padW)</span></code>. Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dW,)</span></code>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.conv_transpose2d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">conv_transpose2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.conv_transpose2d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>This operator supports <span class="xref std std-ref">TensorFloat32</span>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span> <span class="pre">True</span></code>. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math notranslate nohighlight">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math notranslate nohighlight">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sH,</span> <span class="pre">sW)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padH,</span> <span class="pre">padW)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padH,</span> <span class="pre">out_padW)</span></code>.
Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dH,</span> <span class="pre">dW)</span></code>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.conv_transpose3d">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">conv_transpose3d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.conv_transpose3d" title="Permalink to this definition"></a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”</p>
<p>This operator supports <span class="xref std std-ref">TensorFloat32</span>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span> <span class="pre">True</span></code>. See <span class="xref std std-doc">/notes/randomness</span> for more information.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math notranslate nohighlight">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kT , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math notranslate nohighlight">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sT,</span> <span class="pre">sH,</span> <span class="pre">sW)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padT,</span> <span class="pre">padH,</span> <span class="pre">padW)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(out_padT,</span> <span class="pre">out_padH,</span> <span class="pre">out_padW)</span></code>. Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math notranslate nohighlight">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.cosine_similarity">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">cosine_similarity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-8</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.cosine_similarity" title="Permalink to this definition"></a></dt>
<dd><p>Returns cosine similarity between <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code>, computed along dim. <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code> must be broadcastable
to a common shape. <code class="docutils literal notranslate"><span class="pre">dim</span></code> refers to the dimension in this common shape. Dimension <code class="docutils literal notranslate"><span class="pre">dim</span></code> of the output is
squeezed (see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.squeeze()</span></code>), resulting in the
output tensor having 1 fewer dimension.</p>
<div class="math notranslate nohighlight">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}\]</div>
<p>Supports <span class="xref std std-ref">type promotion</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x1</strong> (<em>Tensor</em>) – First input.</p></li>
<li><p><strong>x2</strong> (<em>Tensor</em>) – Second input.</p></li>
<li><p><strong>dim</strong> (<em>int</em><em>, </em><em>optional</em>) – Dimension along which cosine similarity is computed. Default: 1</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.pdist">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">pdist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.pdist" title="Permalink to this definition"></a></dt>
<dd><p>Computes the p-norm distance between every pair of row vectors in the input.
This is identical to the upper triangular portion, excluding the diagonal, of
<cite>torch.norm(input[:, None] - input, dim=2, p=p)</cite>. This function will be faster
if the rows are contiguous.</p>
<p>If input has shape <span class="math notranslate nohighlight">\(N \times M\)</span> then the output will have shape
<span class="math notranslate nohighlight">\(\frac{1}{2} N (N - 1)\)</span>.</p>
<p>This function is equivalent to <cite>scipy.spatial.distance.pdist(input,
‘minkowski’, p=p)</cite> if <span class="math notranslate nohighlight">\(p \in (0, \infty)\)</span>. When <span class="math notranslate nohighlight">\(p = 0\)</span> it is
equivalent to <cite>scipy.spatial.distance.pdist(input, ‘hamming’) * M</cite>.
When <span class="math notranslate nohighlight">\(p = \infty\)</span>, the closest scipy function is
<cite>scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math notranslate nohighlight">\(N \times M\)</span>.</p></li>
<li><p><strong>p</strong> – p value for the p-norm distance to calculate between each vector pair
<span class="math notranslate nohighlight">\(\in [0, \infty]\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.pixel_shuffle">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">pixel_shuffle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upscale_factor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.pixel_shuffle" title="Permalink to this definition"></a></dt>
<dd><p>Rearranges elements in a tensor of shape <span class="math notranslate nohighlight">\((*, C \times r^2, H, W)\)</span> to a
tensor of shape <span class="math notranslate nohighlight">\((*, C, H \times r, W \times r)\)</span>, where r is the <code class="xref py py-attr docutils literal notranslate"><span class="pre">upscale_factor</span></code>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">PixelShuffle</span></code> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – the input tensor</p></li>
<li><p><strong>upscale_factor</strong> (<em>int</em>) – factor to increase spatial resolution by</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.pixel_unshuffle">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">pixel_unshuffle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">downscale_factor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.pixel_unshuffle" title="Permalink to this definition"></a></dt>
<dd><p>Reverses the <code class="xref py py-class docutils literal notranslate"><span class="pre">PixelShuffle</span></code> operation by rearranging elements in a
tensor of shape <span class="math notranslate nohighlight">\((*, C, H \times r, W \times r)\)</span> to a tensor of shape
<span class="math notranslate nohighlight">\((*, C \times r^2, H, W)\)</span>, where r is the <code class="xref py py-attr docutils literal notranslate"><span class="pre">downscale_factor</span></code>.</p>
<p>See <code class="xref py py-class docutils literal notranslate"><span class="pre">PixelUnshuffle</span></code> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – the input tensor</p></li>
<li><p><strong>downscale_factor</strong> (<em>int</em>) – factor to increase spatial resolution by</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pixel_unshuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 9, 4, 4])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.relu_">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">relu_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.relu_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.rrelu_">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">rrelu_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lower</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.</span> <span class="pre">/</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.</span> <span class="pre">/</span> <span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.rrelu_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-func docutils literal notranslate"><span class="pre">rrelu()</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.selu_">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">selu_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.selu_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-func docutils literal notranslate"><span class="pre">selu()</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="btorch.nn.functional.threshold_">
<span class="sig-prename descclassname"><span class="pre">btorch.nn.functional.</span></span><span class="sig-name descname"><span class="pre">threshold_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#btorch.nn.functional.threshold_" title="Permalink to this definition"></a></dt>
<dd><p>In-place version of <code class="xref py py-func docutils literal notranslate"><span class="pre">threshold()</span></code>.</p>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Brian.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>